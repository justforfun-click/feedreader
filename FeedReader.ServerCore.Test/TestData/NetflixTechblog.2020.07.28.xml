<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:cc="http://cyber.law.harvard.edu/rss/creativeCommonsRssModule.html">
    <channel>
        <title><![CDATA[Netflix TechBlog - Medium]]></title>
        <description><![CDATA[Learn about Netflix’s world class engineering efforts, company culture, product developments and more. - Medium]]></description>
        <link>https://netflixtechblog.com?source=rss----2615bd06b42e---4</link>
        <image>
            <url>https://cdn-images-1.medium.com/proxy/1*TGH72Nnw24QL3iV9IOm4VA.png</url>
            <title>Netflix TechBlog - Medium</title>
            <link>https://netflixtechblog.com?source=rss----2615bd06b42e---4</link>
        </image>
        <generator>Medium</generator>
        <lastBuildDate>Tue, 28 Jul 2020 18:46:50 GMT</lastBuildDate>
        <atom:link href="https://netflixtechblog.com/feed" rel="self" type="application/rss+xml"/>
        <webMaster><![CDATA[yourfriends@medium.com]]></webMaster>
        <atom:link href="http://medium.superfeedr.com" rel="hub"/>
        <item>
            <title><![CDATA[Machine Learning for a Better Developer Experience]]></title>
            <link>https://netflixtechblog.com/machine-learning-for-a-better-developer-experience-1e600c69f36c?source=rss----2615bd06b42e---4</link>
            <guid isPermaLink="false">https://medium.com/p/1e600c69f36c</guid>
            <category><![CDATA[developer-productivity]]></category>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[neural-networks]]></category>
            <category><![CDATA[data-science]]></category>
            <dc:creator><![CDATA[Netflix Technology Blog]]></dc:creator>
            <pubDate>Tue, 21 Jul 2020 02:47:27 GMT</pubDate>
            <atom:updated>2020-07-28T16:15:50.370Z</atom:updated>
            <content:encoded><![CDATA[<p><a href="https://www.linkedin.com/in/skirdey/">Stanislav Kirdey</a>, <a href="https://www.linkedin.com/in/fwhigh/">William High</a></p><p>Imagine having to go through 2.5GB (not often, but does happen time to time) log entries from a failed software build — 3 million lines — to search for a bug or a regression that happened on line 1M. It’s probably not even doable manually! However, one smart approach to make it tractable might be to <a href="https://en.wikipedia.org/wiki/Diff">diff</a> the lines against a recent successful build, with the hope that the bug produces unusual lines in the logs.</p><p>Standard <a href="https://en.wikipedia.org/wiki/MD5">md5</a> diff would run quickly but still produce at least hundreds of thousands candidate lines to look through because it surfaces character-level differences between lines. Fuzzy diffing using k-nearest neighbors clustering from machine learning (the kind of thing <a href="https://help.sumologic.com/05Search/LogReducehttps://help.sumologic.com/05Search/LogReduce">logreduce</a> does) produces around 40,000 candidate lines but takes an hour to complete. Our solution produces 20,000 candidate lines in 20 min of computing — and thanks to the magic of open source, it’s only about a hundred lines of Python code.</p><p>The application is a combination of <a href="https://en.wikipedia.org/wiki/Word_embedding">neural embeddings</a>, which encode the semantic information in words and sentences, and <a href="http://tylerneylon.com/a/lsh1/">locality sensitive hashing</a>, which efficiently assigns approximately nearby items to the same buckets and faraway items to different buckets. Combining embeddings with LSH is a great idea that appears to be <a href="https://www.researchgate.net/publication/272821025_A_Neural_Network_Model_for_Large-Scale_Stream_Data_Learning_Using_Locally_Sensitive_Hashing">less</a> <a href="https://sites.cs.ucsb.edu/~tyang/papers/www2019.pdf">than</a> a <a href="https://towardsdatascience.com/fast-near-duplicate-image-search-using-locality-sensitive-hashing-d4c16058efcb">decade</a> <a href="https://towardsdatascience.com/finding-similar-images-using-deep-learning-and-locality-sensitive-hashing-9528afee02f5">old</a>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/600/1*3K2E-ezzvHnbxQ6MTR42bA.jpeg" /></figure><blockquote>Note — we used Tensorflow 2.2 on CPU with eager execution for transfer learning and scikit-learn NearestNeighbor for k-nearest-neighbors. There are <a href="https://github.com/erikbern/ann-benchmarks">sophisticated approximate nearest neighbors implementations</a> that would be better for a model-based nearest neighbors solution.</blockquote><h3>What embeddings are and why we needed them</h3><p>Assembling a <a href="https://en.wikipedia.org/wiki/One-hot">k-hot bag-of-words</a> is a typical (useful!) starting place for deduplication, search, and similarity problems around un- or semi-structured text. This type of bag-of-words encoding looks like a dictionary with individual words and their counts. Here’s what it would look like for the sentence “log in error, check log”.</p><blockquote>{“log”: 2, “in”: 1, “error”: 1, “check”: 1}</blockquote><p>This encoding can also be represented using a vector where the index corresponds to a word and the value is the count. Here is “log in error, check log” as a vector, where the first entry is reserved for “log” word counts, the second for “in” word counts, and so forth.</p><blockquote>[2, 1, 1, 1, 0, 0, 0, 0, 0, …]</blockquote><p>Notice that the vector consists of many zeros. Zero-valued entries represent all the other words in the dictionary that were not present in that sentence. The total number of vector entries possible, or <em>dimensionality</em> of the vector, is the size of your language’s dictionary, which is often millions or more but down to hundreds of thousands with some <a href="https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html">clever</a> <a href="https://en.wikipedia.org/wiki/Feature_hashing#Feature_vectorization_using_hashing_trick">tricks</a>.</p><p>Now let’s look at the dictionary and vector representations of “problem authenticating”. The words corresponding to the first five vector entries do not appear at all in the new sentence.</p><blockquote>{“problem”: 1, “authenticating”: 1}<br>[0, 0, 0, 0, 1, 1, 0, 0, 0, …]</blockquote><p>These two sentences are semantically similar, which means they mean essentially the same thing, but lexically are as different as they can be, which is to say they have no words in common. In a fuzzy diff setting, we might want to say that these sentences are too similar to highlight, but md5 and k-hot document encoding with kNN do not support that.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*culQ8vH4Nl_Hiz50" /><figcaption>Erica Sinclair is helping us get LSH probabilities right</figcaption></figure><p>Dimensionality reduction uses linear algebra or artificial neural networks to place semantically similar words, sentences, and log lines near to each other in a new vector space, using representations known as embeddings. In our example, “log in error, check log” might have a five-dimensional embedding vector</p><blockquote>[0.1, 0.3, -0.5, -0.7, 0.2]</blockquote><p>and “problem authenticating” might be</p><blockquote>[0.1, 0.35, -0.5, -0.7, 0.2]</blockquote><p>These embedding vectors are near to each other by distance measures like <a href="https://en.wikipedia.org/wiki/Cosine_similarity">cosine similarity</a>, unlike their k-hot bag-of-word vectors. Dense, low dimensional representations are really useful for short documents, like lines of a build or a system log.</p><p>In reality, you’d be replacing the thousands or more dictionary dimensions with just 100 information-rich embedding dimensions (not five). State-of-the-art approaches to dimensionality reduction include singular value decomposition of a word co-occurrence matrix (<a href="https://nlp.stanford.edu/pubs/glove.pdf">GloVe</a>) and specialized neural networks (<a href="https://arxiv.org/pdf/1310.4546.pdf">word2vec</a>, <a href="https://arxiv.org/pdf/1810.04805.pdf">BERT</a>, <a href="https://arxiv.org/pdf/1802.05365.pdf">ELMo</a>).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*KXCG2d0cqueTJN0_" /><figcaption>Erica Sinclair and locality-preserving hashing</figcaption></figure><h3>What about clustering? Back to the build log application</h3><p>We joke internally that Netflix is a log-producing service that sometimes streams videos. We deal with hundreds of thousands of requests per second in the fields of exception monitoring, log processing, and stream processing. Being able to scale our NLP solutions is just a must-have if we want to use applied machine learning in telemetry and logging spaces. This is why we cared about scaling our text deduplication, semantic similarity search, and textual outlier detection — there is no other way if the business problems need to be solved in real-time.</p><p>Our diff solution involves embedding each line into a low dimensional vector and (optionally “fine-tuning” or updating the embedding model at the same time), assigning it to a cluster, and identifying lines in different clusters as “different”. <a href="https://en.wikipedia.org/wiki/Locality-sensitive_hashing">Locality sensitive hashing</a> is a probabilistic algorithm that permits constant time cluster assignment and near-constant time nearest neighbors search.</p><p>LSH works by mapping a vector representation to a scalar number, or more precisely a collection of scalars. While standard hashing algorithms aim to avoid <a href="https://medium.com/@glaslos/locality-sensitive-fuzzy-hashing-66127178ebdc">collisions</a> between any two inputs that are not the same, LSH aims to avoid collisions if the inputs are far apart and <em>promote</em> them if they are different but near to each other in the vector space.</p><p>The embedding vector for “log in error, check log” might be mapped to binary number 01 — and 01 then represents the cluster. The embedding vector for “problem authenticating” would with high probability be mapped to the same binary number, 01. This is how LSH enables fuzzy matching, and the inverse problem, fuzzing diffing. Early applications of LSH were over high dimensional bag-of-words vector spaces — we couldn’t think of any reason it wouldn’t work on embedding spaces just as well, and there are signs that <a href="https://towardsdatascience.com/finding-similar-images-using-deep-learning-and-locality-sensitive-hashing-9528afee02f5">others</a> <a href="https://towardsdatascience.com/fast-near-duplicate-image-search-using-locality-sensitive-hashing-d4c16058efcb">have</a> <a href="https://sites.cs.ucsb.edu/~tyang/papers/www2019.pdf">had</a> the same thought.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/600/1*j127n3uIrCHLtmAr036FcA.jpeg" /><figcaption>Using LSH to place characters in the same bucket but in the Upside Down.</figcaption></figure><p>The work we did on applying LSH and neural embeddings in-text outlier detection on build logs now allows an engineer to look through a small fraction of the log’s lines to identify and fix errors in potentially business-critical software, and it also allows us to achieve semantic clustering of almost any log line in real-time.</p><p>We now bring this benefit from semantic LSH to every build at Netflix. The semantic part lets us group seemingly dissimilar items based on their meanings and surface them in outlier reports.</p><h3>A few examples</h3><p>This is our favorite example of a semantic diff, from 6,892 lines to just 3.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*-cNotCXqOZShlaX9" /></figure><p>Another example, this build produced 6,044 lines, but only 171 were left in the report. And the main issue surfaced almost immediately on line 4,036.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*0itXKh4lFiaoxIa5bq3WXw.png" /><figcaption>Quickly parse 171 lines, instead 6,044</figcaption></figure><p>Coming back to the example in the beginning, how did we end up with such large logs in builds? Some of our thousands of build jobs <a href="https://netflixtechblog.com/bringing-rich-experiences-to-memory-constrained-tv-devices-6de771eabb16">stress tests against consumer electronics</a> where they run with trace mode. The amount of data they produce is hard to consume without any pre-processing. One example on the lighter end drops from 91,366 to 455 lines to parse.</p><blockquote>compression ratio: 91,366 / 455 = 200x</blockquote><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*dT33frVi2qTeDrWFJiImNQ.png" /><figcaption>Quickly parse 455 lines, instead of 91,366</figcaption></figure><p>There are various examples that capture also semantic differences across many different frameworks, languages, and build scenarios.</p><h3>Conclusion</h3><p>The mature state of open source transfer learning data products and SDKs has allowed us to solve semantic nearest neighbor search via LSH in remarkably few lines of code. We became especially interested in investigating the special benefits that transfer learning and fine-tuning might bring to the application. We’re excited to have an opportunity to solve such problems and to help people do what they do better and faster than before.</p><p>We hope you’ll consider joining Netflix and becoming one of the stunning colleagues whose life we make easier with machine learning. Inclusion is a core Netflix value and we are particularly interested in fostering a diversity of perspectives on our technical teams. So if you are in analytics, engineering, data science, or any other field and have a background that is atypical for the industry we’d especially like to hear from you!</p><p>If you have any questions about opportunities at Netflix, please reach out to the authors on LinkedIn.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=1e600c69f36c" width="1" height="1"><hr><p><a href="https://netflixtechblog.com/machine-learning-for-a-better-developer-experience-1e600c69f36c">Machine Learning for a Better Developer Experience</a> was originally published in <a href="https://netflixtechblog.com">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Empowering the Visual Effects Community with the NetFX Platform]]></title>
            <description><![CDATA[<div class="medium-feed-item"><p class="medium-feed-snippet">The cloud-based platform allows vendors, artists and creators to connect and collaborate on visual effects (VFX) from anywhere in the&#x2026;</p><p class="medium-feed-link"><a href="https://netflixtechblog.com/empowering-the-visual-effects-community-with-the-netfx-platform-35fdf604909c?source=rss----2615bd06b42e---4">Continue reading on Netflix TechBlog »</a></p></div>]]></description>
            <link>https://netflixtechblog.com/empowering-the-visual-effects-community-with-the-netfx-platform-35fdf604909c?source=rss----2615bd06b42e---4</link>
            <guid isPermaLink="false">https://medium.com/p/35fdf604909c</guid>
            <category><![CDATA[visual-effects]]></category>
            <dc:creator><![CDATA[Netflix Technology Blog]]></dc:creator>
            <pubDate>Fri, 17 Jul 2020 16:01:01 GMT</pubDate>
            <atom:updated>2020-07-17T17:32:06.853Z</atom:updated>
        </item>
        <item>
            <title><![CDATA[Byte Down: Making Netflix’s Data Infrastructure Cost-Effective]]></title>
            <link>https://netflixtechblog.com/byte-down-making-netflixs-data-infrastructure-cost-effective-fee7b3235032?source=rss----2615bd06b42e---4</link>
            <guid isPermaLink="false">https://medium.com/p/fee7b3235032</guid>
            <category><![CDATA[cloud-storage]]></category>
            <category><![CDATA[data]]></category>
            <category><![CDATA[data-infrastructure]]></category>
            <category><![CDATA[aws]]></category>
            <category><![CDATA[netflix]]></category>
            <dc:creator><![CDATA[Netflix Technology Blog]]></dc:creator>
            <pubDate>Wed, 08 Jul 2020 21:59:29 GMT</pubDate>
            <atom:updated>2020-07-11T01:21:20.868Z</atom:updated>
            <content:encoded><![CDATA[<p>By <a href="https://www.linkedin.com/in/torio-risianto/">Torio Risianto</a>, <a href="https://www.linkedin.com/in/bhargavi-reddy-dokuru-4aa2449a/">Bhargavi Reddy</a>, <a href="https://www.linkedin.com/in/tanvi-sahni-71886519/">Tanvi Sahni</a>, <a href="https://www.linkedin.com/in/cloudpark/">Andrew Park</a></p><h3>Background on data efficiency</h3><p>At Netflix, we invest heavily in our data infrastructure which is composed of dozens of data platforms, hundreds of data producers and consumers, and petabytes of data.</p><p>At many other organizations, an effective way to manage data infrastructure costs is to set budgets and other heavy guardrails to limit spending. However, due to the highly distributed nature of our data infrastructure and our emphasis on <a href="https://jobs.netflix.com/culture">freedom and responsibility</a>, those processes are counter-cultural and ineffective.</p><p>Our efficiency approach, therefore, is to provide cost transparency and place the efficiency context as close to the decision-makers as possible. Our highest leverage tool is a custom dashboard that serves as a feedback loop to data producers and consumers — it is the single holistic source of truth for cost and usage trends for Netflix’s data users. This post details our approach and lessons learned in creating our data efficiency dashboard.</p><h3>Netflix’s data platform landscape</h3><p>Netflix’s data platforms can be broadly classified as data at rest and data in motion systems. Data at rest stores such as S3 Data Warehouse, Cassandra, Elasticsearch, etc. physically store data and the infrastructure cost is primarily attributed to storage. Data in motion systems such as <a href="https://netflixtechblog.com/keystone-real-time-stream-processing-platform-a3ee651812a">Keystone</a>, <a href="https://netflixtechblog.com/stream-processing-with-mantis-78af913f51a6">Mantis</a>, Spark, Flink, etc. contribute to data infrastructure compute costs associated with processing transient data. Each data platform contains thousands of distinct data objects (i.e. resources), which are often owned by various teams and data users.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*khKMJgFrd0BHWiHm" /></figure><h3>Creating usage and cost visibility</h3><p>To get a unified view of cost for each team, we need to be able to aggregate costs across all these platforms but also, retaining the ability to break it down by a meaningful resource unit (table, index, column family, job, etc).</p><h4>Data flow</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/889/0*FLWeGZlVX20JBLnx" /></figure><pre><a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/storage-inventory.html"><strong>S3 Inventory</strong></a>: Provides a list of objects and their corresponding metadata like the size in bytes for S3 buckets which are configured to generate the inventory lists.</pre><pre><strong>Netflix Data Catalog (NDC)</strong>: In-house federated metadata store which represents a single comprehensive knowledge base for all data resources at Netflix.</pre><pre><a href="https://github.com/Netflix/atlas/wiki"><strong>Atlas</strong></a>: Monitoring system which generates operational metrics for a system (CPU usage, memory usage, network throughput, etc.)</pre><h4>Cost calculations and business logic</h4><p>As the source of truth for cost data, <a href="https://aws.amazon.com/aws-cost-management/aws-cost-and-usage-reporting/">AWS billing</a> is categorized by service (EC2, S3, etc) and can be allocated to various platforms based on AWS tags. However, this granularity is not sufficient to provide visibility into infrastructure costs by data resource and/or team. We have used the following approach to further allocate these costs:</p><p><strong>EC2-based platforms:</strong> Determine bottleneck metrics for the platform, namely CPU, memory, storage, IO, throughput, or a combination. For example, Kafka data streams are typically network bound, whereas spark jobs are typically CPU and memory bound. Next, we identified the consumption of bottleneck metrics per data resource using Atlas, platform logs, and various REST APIs. Cost is allocated based on the consumption of bottleneck metrics per resource (e.g., % CPU utilization for spark jobs). The detailed calculation logic for platforms can vary depending on their architecture. The following is an example of cost attributions for jobs running in a CPU-bound compute platform:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*lkczEbIsUZlfHDBT" /></figure><p><strong>S3-based platforms</strong>: We use AWS’s S3 Inventory (which has object level granularity) in order to map each S3 prefix to the corresponding data resource (e.g. hive table). We then translate storage bytes per data resource to cost based on S3 storage prices from AWS billing data.</p><h4>Dashboard view</h4><p>We use a <a href="https://druid.apache.org/">druid</a>-backed custom dashboard to relay cost context to teams. The primary target audiences for our cost data are the engineering and data science teams as they have the best context to take action based on such information. In addition, we provide cost context at a higher level for engineering leaders. Depending on the use case, the cost can be grouped based on the data resource hierarchy or org hierarchy. Both snapshots and time-series views are available.</p><p><strong><em>Note: The following snippets containing costs, comparable business metrics, and job titles do not represent actual data and are for ILLUSTRATIVE purposes only.</em></strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*gBNDai4RhWEmCUzh" /><figcaption><strong><em>Illustrative summary facts showing annualized costs and comparable business metrics</em></strong></figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*QbfRZi7Qyp6yIIR1" /><figcaption><strong><em>Illustrative annualized data cost split by organization hierarchy</em></strong></figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*FK_TODevm4L_Zryx" /><figcaption><strong><em>Illustrative annualized data cost split by resource hierarchy for a specific team</em></strong></figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*xZYhfLBiiDAkp7i-" /><figcaption><strong><em>Illustrative time-series showing week over week cost (annualized) for a specific team by platform</em></strong></figcaption></figure><h3>Automated storage recommendations — Time to live (TTL)</h3><p>In select scenarios where the engineering investment is worthwhile, we go beyond providing transparency and provide optimization recommendations. Since data storage has a lot of usage and cost momentum (i.e. save-and-forget build-up), we automated the analysis that determines the optimal duration of storage (TTL) based on data usage patterns. So far, we have enabled TTL recommendations for our S3 big data warehouse tables.</p><p>Our big data warehouse allows individual owners of tables to choose the length of retention. Based on these retention values, data stored in date- partitioned S3 tables are cleaned up by a data janitor process which drops partitions older than the TTL value on a daily basis. Historically most data owners did not have a good way of understanding usage patterns in order to decide optimal TTL.</p><h4>Data flow</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/913/0*hF7vr4AFELdQrR9s" /></figure><pre><a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/ServerLogs.html"><strong>S3 Access logs</strong></a>: AWS generated logging for any S3 requests made which provide detailed records about what S3 prefix was accessed, time of access, and other useful information.</pre><pre><strong>Table Partition Metadata</strong>: Generated from an in-house metadata layer (<a href="https://netflixtechblog.com/metacat-making-big-data-discoverable-and-meaningful-at-netflix-56fb36a53520">Metacat</a>) which maps a hive table and its partitions to a specific underlying S3 location and stores this metadata. This is useful to map the S3 access logs to the DW table which was accessed in the request.</pre><pre><strong>Lookback days</strong>: Difference between the date partition accessed and the date when the partition was accessed.</pre><h4>Cost calculations and business logic</h4><p>The largest S3 storage cost comes from transactional tables, which are typically partitioned by date. Using S3 access logs and S3 prefix-to-table-partition mapping, we are able to determine which date partitions are accessed on any given day. Next, we look at access(read/write) activities in the last 180 days and identify the max lookback days. This maximum value of lookback days determines the ideal TTL of a given table. In addition, we calculate the potential annual savings that can be realized (based on today’s storage level) based on the optimal TTL.</p><h4>Dashboard view</h4><p>From the dashboard, data owners can look at the detailed access patterns, recommended vs. current TTL values, as well as the potential savings.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*W7Wk_OOUYMtY4O-R" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*6FDmqw0ehXbkT5m2" /><figcaption><strong><em>An illustrative example of a table with sub-optimal TTL</em></strong></figcaption></figure><h3>Communication and alerting users</h3><p>Checking data costs should not be part of any engineering team’s daily job, especially those with insignificant data costs. To that regard, we invested in email push notifications to increase data cost awareness among teams with significant data usage. Similarly, we send automated TTL recommendations only for tables with material cost-saving potentials. Currently, these emails are sent monthly.</p><h3>Learnings and challenges</h3><h4>Identifying and maintaining metadata of assets is critical for cost allocation</h4><p>What is a resource? What is the complete set of data resources we own?<br>These questions form the primary building blocks of cost efficiency and allocation. We are extracting metadata for a myriad of platforms across in-motion and at-rest systems as described earlier. Different platforms store their resource metadata in different ways. To address this, Netflix is building a metadata store called the Netflix Data Catalog (NDC). NDC enables easier data access and discovery to support data management requirements for both existing and new data. We use the NDC as the starting point for cost calculations. Having a federated metadata store ensures that we have a universally understood and accepted concept of defining what resources exist and which resources are owned by individual teams.</p><h4>Time trends are challenging</h4><p>Time trends carry a much higher maintenance burden than point-in-time snapshots. In the case of data inconsistencies and latencies in ingestion, showing a consistent view over time is often challenging. Specifically, we dealt with the following two challenges:</p><ul><li><strong>Changes in resource ownership:</strong> for a point-in-time snapshot view, this change should be automatically reflected. However, for a time series view, any change in the ownership should also be reflected in historical metadata as well.</li><li><strong>Loss of state in case of data issues</strong>: resource metadata is extracted from a variety of sources many of which are API extractions, it’s possible to lose state in case of job failures during data ingestion time. API extractions in general have drawbacks because the data is transient. It’s important to explore alternatives like pumping events to Keystone so that we can persist data for a longer period.</li></ul><h3>Conclusion</h3><p>When faced with a myriad of data platforms with a highly distributed, decentralized data user base, consolidating usage and cost context to create feedback loops via dashboards provide great leverage in tackling efficiency. When reasonable, creating automated recommendations to further reduce the efficiency burden is warranted — in our case, there was high ROI in data warehouse table retention recommendations. So far, these dashboards and TTL recommendations have contributed to over a 10% decrease in our data warehouse storage footprint.</p><h3>What’s next?</h3><p>In the future, we plan to further push data efficiency by using different storage classes for resources based on usage patterns as well as identifying and aggressively deleting upstream and downstream dependencies of unused data resources.</p><p><em>Interested in working with large scale data? Platform Data Science &amp; Engineering is </em><a href="https://jobs.netflix.com/search?q=%22platform%20data%20science%20%26%20engineering%22&amp;team=Data%20Science%20and%20Engineering"><em>hiring</em></a><em>!</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=fee7b3235032" width="1" height="1"><hr><p><a href="https://netflixtechblog.com/byte-down-making-netflixs-data-infrastructure-cost-effective-fee7b3235032">Byte Down: Making Netflix’s Data Infrastructure Cost-Effective</a> was originally published in <a href="https://netflixtechblog.com">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Netflix Studio Engineering Overview]]></title>
            <link>https://netflixtechblog.com/netflix-studio-engineering-overview-ed60afcfa0ce?source=rss----2615bd06b42e---4</link>
            <guid isPermaLink="false">https://medium.com/p/ed60afcfa0ce</guid>
            <category><![CDATA[innovation]]></category>
            <category><![CDATA[production]]></category>
            <category><![CDATA[studio]]></category>
            <category><![CDATA[engineering]]></category>
            <category><![CDATA[entertainment]]></category>
            <dc:creator><![CDATA[Netflix Technology Blog]]></dc:creator>
            <pubDate>Tue, 30 Jun 2020 17:36:41 GMT</pubDate>
            <atom:updated>2020-06-30T17:36:41.323Z</atom:updated>
            <content:encoded><![CDATA[<p>By <a href="https://www.linkedin.com/in/surban/">Steve Urban</a>, <a href="https://www.linkedin.com/in/connectwithsridhar/">Sridhar Seetharaman</a>, <a href="https://www.linkedin.com/in/shilpamotukuri/">Shilpa Motukuri</a>, <a href="https://www.linkedin.com/in/tommack8/">Tom Mack</a>, <a href="https://www.linkedin.com/in/erikstrauss/">Erik Strauss</a>, <a href="https://www.linkedin.com/in/hemamalinikannan/">Hema Kannan</a>, <a href="https://www.linkedin.com/in/cjbarkbark/">CJ Barker</a></p><p>Netflix is revolutionizing the way a modern studio operates. Our mission in <strong>Studio Engineering</strong> is to build a unified, global, and digital studio that powers the effective production of amazing content.</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FuoHeQ3uzPWw%3Ffeature%3Doembed&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DuoHeQ3uzPWw&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FuoHeQ3uzPWw%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/4cc867c76d270e5f460748ef6ee0bee7/href">https://medium.com/media/4cc867c76d270e5f460748ef6ee0bee7/href</a></iframe><p>Netflix produces some of the world’s most beloved and award-winning films and series, including The Irishman, The Crown, La Casa de Papel, Ozark, and Tiger King. In an effort to effectively and efficiently produce this content we are looking to improve and automate many areas of the production process. We combine our entertainment knowledge and our technical expertise to provide innovative technical solutions from the initial pitch of an idea to the moment our members hit play.</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FpDu8Ccpr6Us%3Ffeature%3Doembed&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DpDu8Ccpr6Us&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FpDu8Ccpr6Us%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube" width="854" height="480" frameborder="0" scrolling="no"><a href="https://medium.com/media/0c79273054173d7d40b8037c34464589/href">https://medium.com/media/0c79273054173d7d40b8037c34464589/href</a></iframe><h3>Why Does Studio Engineering Exist?</h3><figure><img alt="We enable Netflix to build a unified, global and digital studio that powers the effective production of amazing content." src="https://cdn-images-1.medium.com/max/648/1*IvXYAWujfxqOnP5fwKwvfg.png" /><figcaption>Studio Engineering’s ‘Why’</figcaption></figure><p>The journey of a Netflix Original title from the moment it first comes to us as a pitch, to that press of the play button is incredibly complex. Producing great content requires a significant amount of coordination and collaboration from Netflix employees and external vendors across the various production phases. This process starts before the deal has been struck and continues all the way through launch on the service, involving people representing finance, scheduling, human resources, facilities, asset delivery, and many other business functions. In this overview, we will shed light on the complexity and magnitude of this journey and update this post with links to deeper technical blogs over time.</p><figure><img alt="Content Lifecycle: Pitch, Development, Production, On-Service" src="https://cdn-images-1.medium.com/max/1024/1*T0Hf8NhPFZdcAzbfBbhUmg.png" /><figcaption>Pitch-to-Play</figcaption></figure><h3>Mission at a Glance</h3><ul><li><strong>Creative pitch</strong>: Combine the best of machine learning and human intuition to help Netflix understand how a proposed title compares to other titles, estimate how many subscribers will enjoy it, and decide whether or not to produce it.</li><li><strong>Business negotiations: </strong>Empower the Netflix Legal team with data to help with deal negotiations and acquisition of rights to produce and stream the content.</li><li><strong>Pre-Production:</strong> Provide solutions to plan for resource needs, and discovery of people and vendors to continue expanding the scale of our productions. Any given production requires the collaboration of hundreds of people with varying expertise, so finding exactly the right people and vendors for each job is essential.</li><li><strong>Production: </strong>Enable content creation from script to screen that optimizes the production process for efficiency and transparency. Free up creative resources to focus on what’s important: producing amazing and entertaining content.</li><li><strong>Post-Production:</strong> Help our creative partners collaborate to refine content into their final vision with digital content logistics and orchestration.</li></ul><h3>What’s Next?</h3><p>Studio Engineering will be publishing a series of articles providing business and technical insights as we further explore the details behind the journey from pitch to play. Stay tuned as we expand on each stage of the content lifecycle over the coming months!</p><p>Here are some related articles to Studio Engineering:</p><ul><li><a href="https://jobs.netflix.com/teams/studio-technologies">Studio Technologies</a></li><li><a href="https://netflixtechblog.com/ready-for-changes-with-hexagonal-architecture-b315ec967749">Ready for changes with Hexagonal Architecture</a></li><li><a href="https://netflixtechblog.com/graphql-search-indexing-334c92e0d8d5">GraphQL Search Indexing</a></li><li><a href="https://netflixtechblog.com/netflix-studio-hack-day-may-2019-b4a0ecc629eb">Netflix Studio Hack Day — May 2019</a></li></ul><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=ed60afcfa0ce" width="1" height="1"><hr><p><a href="https://netflixtechblog.com/netflix-studio-engineering-overview-ed60afcfa0ce">Netflix Studio Engineering Overview</a> was originally published in <a href="https://netflixtechblog.com">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Keeping Customers Streaming — The Centralized Site Reliability Practice at Netflix]]></title>
            <link>https://netflixtechblog.com/keeping-customers-streaming-the-centralized-site-reliability-practice-at-netflix-205cc37aa9fb?source=rss----2615bd06b42e---4</link>
            <guid isPermaLink="false">https://medium.com/p/205cc37aa9fb</guid>
            <category><![CDATA[incident-response]]></category>
            <category><![CDATA[sre]]></category>
            <category><![CDATA[incident-management]]></category>
            <category><![CDATA[reliability]]></category>
            <category><![CDATA[site-reliability]]></category>
            <dc:creator><![CDATA[Netflix Technology Blog]]></dc:creator>
            <pubDate>Wed, 27 May 2020 16:59:11 GMT</pubDate>
            <atom:updated>2020-05-27T17:44:15.840Z</atom:updated>
            <content:encoded><![CDATA[<h3>Keeping Customers Streaming — The Centralized Site Reliability Practice at Netflix</h3><p>By <a href="https://www.linkedin.com/in/hajacobs/">Hank Jacobs</a>, Senior Site Reliability Engineer on CORE</p><p>We’re privileged to be in the business of bringing joy to our customers at Netflix. Whether it’s a compelling new series or an innovative product feature, we strive to provide a best-in-class service that people love and can enjoy anytime, anywhere. A key underpinning to keeping our customers happy and streaming is a strong focus on reliability.</p><p>Reliability, formally speaking, is the ability of a system to function under stated conditions for a period of time. Put simply, reliability means a system should work and continue working. From <a href="https://netflixtechblog.com/fit-failure-injection-testing-35d8e2a9bb2">failure injection testing</a> to regularly exercising our <a href="https://netflixtechblog.com/project-nimble-region-evacuation-reimagined-d0d0568254d4">region evacuation</a> abilities, Netflix engineers invest a lot in ensuring the services that comprise Netflix are <a href="https://www.youtube.com/watch?v=GFvgOumfuWc&amp;t=241s">robust and reliable</a>. Many teams contribute to the reliability of Netflix and own the reliability of their service or area of expertise. The Critical Operations and Reliability Engineering team at Netflix (CORE) is responsible for the reliability of the Netflix service as a whole.</p><p>CORE is a team consisting of Site Reliability Engineers, Applied Resilience Engineers, and Performance Engineers. Our group is responsible for the reliability of business-critical operations. Unlike most SRE teams, we do not own or operate any customer-serving services nor do we routinely make production code changes, build infrastructure, or embed on service teams. Our primary focus is ensuring Netflix stays up. Practically speaking, this includes activities such as systemic risk identification, handling the lifecycle of an incident, and reliability consulting.</p><p>Teams at Netflix follow the <a href="https://netflixtechblog.com/full-cycle-developers-at-netflix-a08c31f83249">service ownership model</a>: they operate what they build. Most of the time, service owners catch issues before they impact customers. Things still occasionally go sideways and incidents happen that impact the customer experience. This is where the CORE team steps in: CORE configures, maintains, and responds to alerts that monitor high-level business KPIs (<a href="https://netflixtechblog.com/sps-the-pulse-of-netflix-streaming-ae4db0e05f8a">stream starts per second</a>, for instance). When one of those alerts fires, the CORE on-call engineer assesses the situation to determine the scope of impact, identify involved services, and engage service owners to assist with mitigation. From there, CORE begins to manage the incident.</p><p>Incident management at Netflix doesn’t follow common management practices like the <a href="https://wiki.en.it-processmaps.com/images/7/73/Incident-management-itil.jpg">ITIL model</a>. In an incident, the CORE on-call engineer generally operates as the Incident Manager. The Incident Manager is responsible for performing or delegating activities such as:</p><ul><li>Coordination — bringing in relevant service owners to help with the investigation and focus on mitigation</li><li>Decision Making — making key choices to facilitate the mitigation and remediation of customer impact (e.g. deciding if we should evacuate a region)</li><li>Scribe — keeping track of incident details such as involved teams, mitigation efforts, graphs of the current impact, etc.</li><li>Technical Sleuthing — assisting the responding service owners with understanding what systems are contributing to the incident</li><li>Liaison — communicating information about the incident across business functions with both internal and external teams as necessary</li></ul><p>Once the customer impact is successfully mitigated, CORE is then responsible for coordinating the post-incident analysis. Analysis comes in many shapes and sizes depending on the impact and uniqueness of the incident, but most incidents go through what we call “memorialization”. This process includes a write-up of what happened, what mitigations took place, and what follow-up work was discussed. For particularly unique, interesting, or impactful incidents, CORE <em>may</em> host an Incident Review or engage in a deeper, long-form investigation. Most post-incident analysis, especially for impactful incidents, is done in partnership with one of CORE’s Applied Resilience Engineers. A key point to emphasize is that all incident analysis work focuses on the <a href="https://en.wikipedia.org/wiki/Sociotechnical_system">sociotechnical</a> aspects of an incident. Consequently, post-incident analysis tends to uncover many practical learnings and improvements for all involved. We frequently socialize these findings outside of those directly involved to help share learnings across the company.</p><p>So what happens when a CORE engineer is not on-call or doing incident analysis? Unsurprisingly, the response varies widely based on the skillset and interests of the individual team member. In broad strokes, examples include:</p><ul><li>Preserving operational visibility and response capabilities — fixing and improving our dashboards, alerts, and automation</li><li>Reliability consulting — discussing various aspects including architectural decisions, systemic observability, application performance, and on-call health training</li><li>Systematic risk identification and mitigation — partner with various teams to identify and fix systematic risks revealed by incidents</li><li>Internal tooling — build and maintain tools that support and augment our incident response capabilities</li><li>Learning and re-learning the changes to a complex, ever-moving system</li><li>Building and maintaining relationships with other teams</li></ul><p>Overall, we’ve found that this form of reliability work best suits the needs and goals of Netflix. Reliability being CORE’s primary focus affords us the bandwidth to both proactively explore potential business-critical risks as well as effectively respond to those risks. Additionally, having a broad view of the system allows us to spot systematic risks as they develop. By being a separate and central team, we can more efficiently share learnings across the larger engineering organization and more easily consult with teams on an ad hoc basis. Ultimately, CORE’s singular focus on reliability empowers us to reveal business-critical sociotechnical risks, facilitate effective responses to those risks and ensure Netflix continues to bring joy to our customers.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=205cc37aa9fb" width="1" height="1"><hr><p><a href="https://netflixtechblog.com/keeping-customers-streaming-the-centralized-site-reliability-practice-at-netflix-205cc37aa9fb">Keeping Customers Streaming — The Centralized Site Reliability Practice at Netflix</a> was originally published in <a href="https://netflixtechblog.com">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Hyper Scale VPC Flow Logs enrichment to provide Network Insight]]></title>
            <link>https://netflixtechblog.com/hyper-scale-vpc-flow-logs-enrichment-to-provide-network-insight-e5f1db02910d?source=rss----2615bd06b42e---4</link>
            <guid isPermaLink="false">https://medium.com/p/e5f1db02910d</guid>
            <category><![CDATA[data-engineering]]></category>
            <category><![CDATA[cloud-networking]]></category>
            <category><![CDATA[big-data]]></category>
            <category><![CDATA[cloud-infrastructure]]></category>
            <category><![CDATA[netflix]]></category>
            <dc:creator><![CDATA[Netflix Technology Blog]]></dc:creator>
            <pubDate>Tue, 26 May 2020 13:38:13 GMT</pubDate>
            <atom:updated>2020-05-26T13:38:13.194Z</atom:updated>
            <content:encoded><![CDATA[<h3>How Netflix is able to enrich VPC Flow Logs at Hyper Scale to provide Network Insight</h3><p>By <a href="https://www.linkedin.com/in/haananth/">Hariharan Ananthakrishnan</a> and <a href="https://www.linkedin.com/in/hangelaho/">Angela Ho</a></p><p>The Cloud Network Infrastructure that Netflix utilizes today is a large distributed ecosystem that consists of specialized functional tiers and services such as DirectConnect, VPC Peering, Transit Gateways, NAT Gateways, etc. While we strive to keep the ecosystem simple, the inherent nature of leveraging a variety of technologies will lead us to complications and challenges such as:</p><ul><li><strong>App Dependencies and Data Flow Mappings: </strong>Without understanding and having visibility into an application’s dependencies and data flows, it is difficult for both service owners and centralized teams to identify systemic issues.</li><li><strong>Pathway Validation: </strong>Netflix velocity of change within the production streaming environment can result in the inability of services to communicate with other resources.</li><li><strong>Service Segmentation: </strong>The ease of the cloud deployments has led to the organic growth of multiple AWS accounts, deployment practices, interconnection practices, etc. Without having network visibility, it’s not possible to improve our reliability, security and capacity posture.</li><li><strong>Network Availability: </strong>The expected continued growth of our ecosystem makes it difficult to understand our network bottlenecks and potential limits we may be reaching.</li></ul><p><strong>Cloud Network Insight </strong>is a suite of solutions that provides both operational and analytical insight into the Cloud Network Infrastructure to address the identified problems. By collecting, accessing and analyzing network data from a variety of sources like VPC Flow Logs, ELB Access Logs, Custom Exporter Agents, etc, we can provide Network Insight to users through multiple data visualization techniques like <a href="https://netflixtechblog.com/lumen-custom-self-service-dashboarding-for-netflix-8c56b541548c">Lumen</a>, <a href="https://github.com/Netflix/atlas">Atlas</a>, etc.</p><h3>VPC Flow Logs</h3><p><a href="https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs.html">VPC Flow Logs</a> is an AWS feature that captures information about the IP traffic going to and from network interfaces in a VPC. At Netflix we publish the Flow Log data to Amazon S3. Flow Logs are enabled tactically on either a VPC or subnet or network interface. A <a href="https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs.html#flow-log-records">flow log record</a> represents a network flow in the VPC. By default, each record captures a network internet protocol (IP) traffic flow (characterized by a 5-tuple on a per network interface basis) that occurs within an aggregation interval.</p><pre>version vpc-id subnet-id instance-id interface-id account-id type <strong>srcaddr dstaddr</strong> srcport dstport <strong>pkt-srcadd</strong>r <strong>pkt-dstaddr </strong>protocol bytes packets start end action tcp-flags log-status</pre><pre>3 vpc-12345678 subnet-012345678 i-07890123456 eni-23456789 123456789010 IPv4 <strong>52.213.180.42 10.0.0.62</strong> 43416 5001 <strong>52.213.180.42 10.0.0.62</strong> 6 568 8 1566848875 1566848933 ACCEPT 2 OK</pre><p>The IP addresses within the Cloud can move from one EC2 instance or <a href="https://netflix.github.io/titus/">Titus</a> container to another over time. To understand the attributes of each IP back to an application metadata Netflix uses <a href="https://www.slideshare.net/AmazonWebServices/a-day-in-the-life-of-a-cloud-network-engineer-at-netflix-net303-reinvent-2017">Sonar</a>. Sonar is an IPv4 and IPv6 address identity tracking service. VPC Flow Logs are enriched using IP Metadata from Sonar as it is ingested.</p><p>With a large ecosystem at Netflix, we receive hundreds of thousands of VPC Flow Log files in S3 each hour. And in order to gain visibility into these logs, we need to somehow ingest and enrich this data.</p><h3>So how do we ingest all these s3 files?</h3><p>At Netflix, we have the option to use Spark as our distributed computing platform. It is easier to tune a large Spark job for a consistent volume of data. As you may know, <a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html">S3 can emit messages</a> when events (such as a file creation events) occur which can be directed into an AWS SQS queue. In addition to the s3 object path, these events also conveniently include file size which allows us to intelligently decide how many messages to grab from the SQS queue and when to stop. What we get is a group of messages representing a set of s3 files which we humorously call “Mouthfuls”. In other words, we are able to ensure that our Spark app does not “eat” more data than it was tuned to handle.</p><p>We named this library Sqooby. It works well for other pipelines that have thousands of files landing in s3 per day. But how does it hold up to the likes of Netflix VPC Flow Logs that has volumes which are orders of magnitude greater? It didn’t. The primary limitation was that AWS SQS queues have a limit of 120 thousand in-flight messages. We found ourselves needing to hold more than 120 thousand messages in flight at a time in order to keep up with the volumes of files.</p><h3>Requirements</h3><p>There are multiple ways you can solve this problem and many technologies to choose from. As with any sustainable engineering design, focusing on simplicity is very important. This means using existing infrastructure and established patterns within the Netflix ecosystem as much as possible and minimizing the introduction of new technologies.</p><p>Equally important is the resilience, recoverability, and supportability of the solution. A malformed file should not hold up or back up the pipeline (resilience). If unexpected environmental factors cause the pipeline to get backed up, it should be able to recover by itself. And excellent logging is needed for debugging purposes and supportability. These characteristics allow for an on-call response time that is relaxed and more in line with traditional big data analytical pipelines.</p><h3>Hyper Scale</h3><p>At Netflix, our <a href="https://jobs.netflix.com/culture">culture</a> gives us the freedom to decide how we solve problems as well as the responsibility of maintaining our solutions so that we may choose wisely. So how did we solve this scale problem that meets all of the above requirements? By applying existing established patterns in our ecosystem on top of Sqooby. In this case, it’s a pattern which generates events (directed into another AWS SQS queue) whenever data lands in a table in a datastore. These events represent a specific cut of data from the table.</p><p>We applied this pattern to the Sqooby log tables which contained information about s3 files for each Mouthful. What we got were events that represented Mouthfuls. Spark could look up and retrieve the data in the s3 files that the Mouthful represented. This intermediate step of persisting Mouthfuls allowed us to easily “eat” through S3 event SQS messages at great speed, converting them to far fewer Mouthful SQS Messages which would each be consumed by a single Spark app instance. Because we ensured that our ingestion pipeline could concurrently write/append to the final VPC Flow Log table, this meant that we could scale out the number of Spark app instances we spin up.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*fKQgIiv-CwNR7pM2" /></figure><h3>Tuning for Hyper Scale</h3><p>On this journey of ingesting VPC flow logs, we found ourselves tweaking configurations in order to tune throughput of the pipeline. We modified the size of each Mouthful and tuned the number of Spark executors per Spark app while being mindful of cluster capacity. We also adjusted the frequency in which Spark app instances are spun up such that any backlog would burn off during a trough in traffic.</p><h3>Summary</h3><p>Providing Network Insight into the Cloud Network Infrastructure using VPC Flow Logs at hyper scale is made possible with the Sqooby architecture. After several iterations of this architecture and some tuning, Sqooby has proven to be able to scale.</p><p>We are currently ingesting and enriching hundreds of thousands of VPC Flow Logs S3 files per hour and providing visibility into our cloud ecosystem. The enriched data allows us to analyze networks across a variety of dimensions (e.g. availability, performance, and security), to ensure applications can effectively deliver their data payload across a globally dispersed cloud-based ecosystem.</p><h3>Special Thanks To</h3><p><a href="https://www.linkedin.com/in/bryankeller2/">Bryan Keller</a>, <a href="https://www.linkedin.com/in/rdblue/">Ryan Blue</a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=e5f1db02910d" width="1" height="1"><hr><p><a href="https://netflixtechblog.com/hyper-scale-vpc-flow-logs-enrichment-to-provide-network-insight-e5f1db02910d">Hyper Scale VPC Flow Logs enrichment to provide Network Insight</a> was originally published in <a href="https://netflixtechblog.com">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How Netflix brings safer and faster streaming experience to the living room on crowded networks…]]></title>
            <link>https://netflixtechblog.com/how-netflix-brings-safer-and-faster-streaming-experience-to-the-living-room-on-crowded-networks-78b8de7f758c?source=rss----2615bd06b42e---4</link>
            <guid isPermaLink="false">https://medium.com/p/78b8de7f758c</guid>
            <category><![CDATA[playback]]></category>
            <category><![CDATA[tls]]></category>
            <category><![CDATA[netflix]]></category>
            <category><![CDATA[security]]></category>
            <category><![CDATA[streaming]]></category>
            <dc:creator><![CDATA[Netflix Technology Blog]]></dc:creator>
            <pubDate>Tue, 21 Apr 2020 02:28:21 GMT</pubDate>
            <atom:updated>2020-04-22T23:24:58.648Z</atom:updated>
            <content:encoded><![CDATA[<h3>How Netflix brings safer and faster streaming experiences to the living room on crowded networks using TLS 1.3</h3><p>By Sekwon Choi</p><p>At Netflix, we are obsessed with the best streaming experiences. We want playback to start instantly and to never stop unexpectedly in any network environment. We are also committed to protecting users’ privacy and service security without sacrificing any part of the playback experience.</p><p>To achieve that, we are efficiently using ABR (adaptive bitrate streaming) for a better playback experience, DRM (Digital Right Management) to protect our service and TLS (Transport Layer Security) to protect customer privacy and to create a safer streaming experience.</p><p>Netflix on consumer electronics devices such as TVs, set-top boxes and streaming sticks was until recently using TLS 1.2 for streaming traffic. Now we support TLS 1.3 for safer and faster experiences.</p><h3>What is TLS?</h3><p>For two parties to communicate securely, a secure channel is necessary. This needs to have the following three properties.</p><ul><li>Authentication: Identity of the communicating party is verified.</li><li>Confidentiality: Data sent over the channel is only visible to the endpoints.</li><li>Integrity: Data sent over the channel cannot be modified by attackers without detection.</li></ul><p>The TLS protocol is designed to provide a secure channel between two peers by providing tools and methods to achieve the above properties.</p><h3>TLS 1.3</h3><p>TLS 1.3 is the latest version of the Transport Layer Security protocol. It is simpler, more secure and more efficient than its predecessor.</p><h4>Perfect Forward Secrecy</h4><p>One thing we believe is very important at Netflix is providing PFS (Perfect Forward Secrecy).</p><p>PFS is a feature of the key exchange algorithm that assures that session keys will not be compromised, even if the server’s private key is compromised. By generating new keys for each session, PFS protects past sessions against the future compromise of secret keys.</p><p>TLS 1.2 supports key exchange algorithms with PFS, but it also allows key exchange algorithms that do not support PFS. Even with the previous version of TLS 1.2, Netflix has always selected a key exchange algorithm that provides PFS such as ECDHE (Elliptic Curve Diffie Hellman Ephemeral). TLS 1.3, however, enforces this concept even more by removing all the key exchange algorithms that do not provide PFS, such as static RSA.</p><h4>Authenticated Encryption</h4><p>For encryption, TLS 1.3 removes all weak ciphers and uses only Authenticated Encryption with Associated Data (AEAD). This assures the confidentiality, integrity, and authenticity of the data. We use AES Galois/Counter Mode, as it also provides good performance and high throughput.</p><h4>Secure Handshake</h4><p>While the above changes are important, the most important change in TLS 1.3 is perhaps its redesign of the handshake protocol.</p><p>The TLS 1.2 handshake was not designed to protect the integrity of the entire handshake. It protected only the part of the handshake after the cipher suite negotiation and this opened up the possibility of downgrade attacks which may allow the attackers to force the use of insecure cipher suites.</p><p>With TLS 1.3, the server signs the entire handshake including the cipher suite negotiation and thus prevents the attacker from downgrading the cipher suite.</p><p>Also in TLS 1.2, extensions were sent in the clear in the ServerHello. Now with TLS 1.3, even extensions are encrypted and all handshake messages after ServerHello are now encrypted.</p><h4>Reduced Handshake</h4><p>TLS 1.2 supports numerous key exchange algorithms, cipher suites and digital signatures, including weak and vulnerable ones. Therefore, it requires more messages to perform a handshake and two network round trips.</p><p>In contrast, the handshake in TLS 1.3 now requires only one round trip, with a simplified design and with all weak and vulnerable algorithms removed.</p><p>In addition, it has a new feature called 0-RTT, or TLS early data, for the resumed handshake. This allows an application to include application data with its initial handshake message, instead of having to wait until the handshake completes.</p><p>At Netflix, by the efficient resumption of the TLS session and careful use of 0-RTT for the streaming data, we can reduce the play delay.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*dwfkh2K4bBsQHdP_rEyH7Q.png" /></figure><h3>A/B Testing Result</h3><p>We were pretty confident that TLS 1.3 would bring us better security from the analysis of its protocol composition, but we did not know how it would perform in the context of streaming.</p><p>Since TLS 1.3’s performance-related feature is the 0-RTT mode with the resumed handshake, our hypothesis is that TLS 1.3 would reduce play delay, as we are no longer required to wait for the handshake to finish and we can instead issue the HTTP request for media data and receive the HTTP response for media data earlier.</p><p>To see the actual performance of TLS 1.3 in the field, we performed an experiment with</p><ul><li>User accounts: half-million user accounts per cell.</li><li>Device type: mid-performance device with Quad ARM core @ 1.7GHz.</li><li>Control cell: TLS 1.2</li><li>Treatment cell: TLS 1.3</li></ul><h4>Play Delay</h4><p>Play Delay is defined by how long it takes for playback to start. Below are the results of the play delay measured in the experiment. The results imply that on slower or congested networks, which can be represented by the quantiles of at least 0.75, TLS 1.3 achieves the largest gains, with improvements across all network conditions.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*So7N5ohUZYZnP2ZfgLyNEg.png" /></figure><p>Below is the time series median play delay graph for this mid-performance device in the field. It also shows that playback starts earlier with TLS 1.3.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*6Hkq1ZCussPA1s-qtcuYOg.png" /></figure><h4>Media Rebuffer</h4><p>At Netflix, we define a media rebuffer as a non-network originated rebuffer. It typically occurs when media data is not processed quickly enough by the device due to the high load on the CPU. Comparing the control cell with TLS 1.2, the experiment cell with TLS 1.3 showed about a 7.4% improvement in media rebuffers. This result implies that using TLS 1.3 with 0-RTT is more efficient and can reduce the CPU load.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/895/1*sZY-mXVAxBMFMp9ZvVl5Aw.png" /></figure><h3>Conclusion</h3><p>From the security analysis, we are confident that TLS 1.3 improves communication security over TLS 1.2. From the field test, we are confident that TLS 1.3 provides us a better streaming experience.</p><p>At the time of writing this article, the Internet is experiencing higher than usual traffic and congestion. We believe saving even small amounts of data and round trips can be meaningful and even better if it also provides a more secure and efficient streaming experience.</p><p>Therefore, we have started deploying TLS 1.3 on newer consumer electronics devices and we are expecting even more devices to be deployed with TLS 1.3 capability in the near future.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=78b8de7f758c" width="1" height="1"><hr><p><a href="https://netflixtechblog.com/how-netflix-brings-safer-and-faster-streaming-experience-to-the-living-room-on-crowded-networks-78b8de7f758c">How Netflix brings safer and faster streaming experience to the living room on crowded networks…</a> was originally published in <a href="https://netflixtechblog.com">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Bringing 4K and HDR to Anime at Netflix with Sol Levante]]></title>
            <description><![CDATA[<div class="medium-feed-item"><p class="medium-feed-image"><a href="https://netflixtechblog.com/bringing-4k-and-hdr-to-anime-at-netflix-with-sol-levante-fa68105067cd?source=rss----2615bd06b42e---4"><img src="https://cdn-images-1.medium.com/max/1352/1*DLKFTW9BT92QOF7jx7uGZA.png" width="1352"></a></p><p class="medium-feed-snippet">By Haruka Miyagawa &amp; Kylee Pe&#xF1;a</p><p class="medium-feed-link"><a href="https://netflixtechblog.com/bringing-4k-and-hdr-to-anime-at-netflix-with-sol-levante-fa68105067cd?source=rss----2615bd06b42e---4">Continue reading on Netflix TechBlog »</a></p></div>]]></description>
            <link>https://netflixtechblog.com/bringing-4k-and-hdr-to-anime-at-netflix-with-sol-levante-fa68105067cd?source=rss----2615bd06b42e---4</link>
            <guid isPermaLink="false">https://medium.com/p/fa68105067cd</guid>
            <category><![CDATA[4k]]></category>
            <category><![CDATA[netflix]]></category>
            <category><![CDATA[anime]]></category>
            <category><![CDATA[animation]]></category>
            <category><![CDATA[hdr]]></category>
            <dc:creator><![CDATA[Netflix Technology Blog]]></dc:creator>
            <pubDate>Thu, 02 Apr 2020 09:18:46 GMT</pubDate>
            <atom:updated>2020-04-02T07:01:00.929Z</atom:updated>
        </item>
        <item>
            <title><![CDATA[SVT-AV1: an open-source AV1 encoder and decoder]]></title>
            <link>https://netflixtechblog.com/svt-av1-an-open-source-av1-encoder-and-decoder-ad295d9b5ca2?source=rss----2615bd06b42e---4</link>
            <guid isPermaLink="false">https://medium.com/p/ad295d9b5ca2</guid>
            <category><![CDATA[open-source]]></category>
            <category><![CDATA[video-encoding]]></category>
            <category><![CDATA[netflix]]></category>
            <category><![CDATA[av1]]></category>
            <category><![CDATA[video-compression]]></category>
            <dc:creator><![CDATA[Netflix Technology Blog]]></dc:creator>
            <pubDate>Fri, 13 Mar 2020 18:29:52 GMT</pubDate>
            <atom:updated>2020-03-13T17:21:00.833Z</atom:updated>
            <content:encoded><![CDATA[<h3>SVT-AV1: open-source AV1 encoder and decoder</h3><h4>by <a href="https://norkin.org/">Andrey Norkin</a>, <a href="https://www.linkedin.com/in/joel-sole-rojals">Joel Sole</a>, <a href="https://www.linkedin.com/in/marianafafonso">Mariana Afonso</a>, Kyle Swanson, <a href="https://www.linkedin.com/in/agataopalach">Agata Opalach</a>, <a href="https://www.linkedin.com/in/anush-moorthy-b8451142">Anush Moorthy</a>, <a href="https://www.linkedin.com/in/anne-aaron">Anne Aaron</a></h4><p>SVT-AV1 is an open-source AV1 codec implementation hosted on GitHub <a href="https://github.com/OpenVisualCloud/SVT-AV1/">https://github.com/OpenVisualCloud/SVT-AV1/</a> under a BSD + patent license. As mentioned in our earlier blog <a href="https://netflixtechblog.com/introducing-svt-av1-a-scalable-open-source-av1-framework-c726cce3103a">post</a>, Intel and Netflix have been collaborating on the SVT-AV1 encoder and decoder framework since August 2018. The teams have been working closely on SVT-AV1 development, discussing architectural decisions, implementing new tools, and improving compression efficiency. Since open-sourcing the project, other partner companies and the open-source community have contributed to SVT-AV1. In this tech blog, we will report the current status of the SVT-AV1 project, as well as the characteristics and performance of the encoder and decoder.</p><h3>SVT-AV1 codebase status</h3><p>The SVT-AV1 repository includes both an AV1 encoder and decoder, which share a significant amount of the code. The SVT-AV1 decoder is fully functional and compliant with the AV1 specification for all three profiles (Main, High, and Professional).</p><p>The SVT-AV1 encoder supports all AV1 tools which contribute to compression efficiency. Compared to the most recent master version of libaom (AV1 reference software), SVT-AV1 is similar in compression efficiency and at the same time achieves significantly lower encoding latency on multi-core platforms when using its inherent parallelization capabilities.</p><p>SVT-AV1 is written in C and can be compiled on major platforms, such as Windows, Linux, and macOS. In addition to the pure C function implementations, which allows for more flexible experimentation, the codec features extensive assembly and intrinsic optimizations for the x86 platform. See the next section for an outline of the main SVT-AV1 features that allow high performance at competitive compression efficiency. SVT-AV1 also includes extensive <a href="https://github.com/OpenVisualCloud/SVT-AV1/tree/master/Docs">documentation</a> on the encoder design targeted to facilitate the onboarding process for new developers.</p><h3>Architectural features</h3><p>One of Intel’s goals for SVT-AV1 development was to create an AV1 encoder that could offer performance and scalability. SVT-AV1 uses parallelization at several stages of the encoding process, which allows it to adapt to the number of available cores, including the newest servers with significant core count. This makes it possible for SVT-AV1 to decrease encoding time while still maintaining compression efficiency.</p><p>The SVT-AV1 encoder uses multi-dimensional (process-, picture/tile-, and segment-based) parallelism, multi-stage partitioning decisions, block-based multi-stage and multi-class mode decisions, and RD-optimized classification to achieve attractive trade-offs between compression and performance. Another feature of the SVT architecture is open-loop hierarchical motion estimation, which makes it possible to decouple the first stage of motion estimation from the rest of the encoding process.</p><h3>Compression efficiency and performance</h3><h4>Encoder performance</h4><p>SVT-AV1 reaches similar compression efficiency as libaom at the slowest speed settings. During the codec development, we have been tracking the compression and encoding results at the <a href="https://videocodectracker.dev/">https://videocodectracker.dev/</a> site. The plot below shows the improvements in the compression efficiency of SVT-AV1 compared to the libaom encoder over time. Note that the libaom compression has also been improving over time, and the plot below represents SVT-AV1 catching up with the moving target. In the plot, the Y-axis shows the additional bitrate in percent needed to achieve similar quality as libaom encoder according to three metrics. The plot shows the results of the 2-pass encoding mode in both codecs. SVT-AV1 uses 4-thread mode, whereas libaom operates in a single-thread mode. The SVT-AV1 results for the 1-pass fixed-QP encoding mode, commonly used in research, are even more competitive, as detailed below.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*WZ1vxLVGroExYsuT" /><figcaption>Reducing BD-rate between SVT-AV1 and libaom in 2-pass encoding mode</figcaption></figure><p>The comparison results of the SVT-AV1 against libaom on <a href="https://tools.ietf.org/html/draft-ietf-netvc-testing-09#section-5.2.5">objective-1-fast</a> test set are presented in the table below. For estimating encoding times, we used Intel(R) Xeon(R) Platinum 8170 CPU @ 2.10GHz machine with 52 physical cores and 96 GB of RAM, with 60 jobs running in parallel. Both codecs use bi-directional hierarchical prediction structure of 16 pictures. The results are presented for 1-pass mode with fixed frame-level QP offsets. A single-threaded compression mode is used. Below, we compute the BD-rates for the various quality metrics: PSNR on all three color planes, VMAF, and MS-SSIM. A negative BD-Rate indicates that the SVT-AV1 encodes produce the same quality with the indicated relative reduction in bitrate. As seen below, SVT-AV1 demonstrates 16.5% decrease in encoding time compared to libaom while being slightly more efficient in compression ability. Note that the encoding times ratio may vary depending on the instruction sets supported by the platform. The results have been obtained on SVT-AV1 cs2 branch (a development branch that is currently being merged into the master, git hash 3a19f29) against the libaom master branch (git hash fe72512). The QP values used to calculate the BD-rates are: 20, 32, 43, 55, 63.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*SV2IFZGukwhCpqnehG8RjQ.png" /><figcaption>BD-rates of SVT-AV1 vs libaom in 1-pass encoding mode with fixed QP offsets. Negative numbers indicate reduction in bitrate needed to reach the same quality level. The overall encoding time difference is change in total CPU time for all sequences and QPs of SVT-AV1 compared to that of libaom.</figcaption></figure><p>*The overall encoding CPU time difference is calculated as change in total CPU time for all sequences and QPs of the test compared to that of the anchor. It is not equal to the average of per sequence values. Per each sequence, the encoding CPU time difference is calculated as change in total CPU time for all QPs for this sequence.</p><p>Since all sequences in the objective-1-fast test set have 60 frames, both codecs use one key frame. The following command line parameters have been used to compare the codecs.</p><p>libaom parameters:</p><pre>--passes=1 --lag-in-frames=25 --auto-alt-ref=1 --min-gf-interval=16 --max-gf-interval=16 --gf-min-pyr-height=4 --gf-max-pyr-height=4 --kf-min-dist=65 --kf-max-dist=65 --end-usage=q --use-fixed-qp-offsets=1 --deltaq-mode=0 --enable-tpl-model=0 --cpu-used=0</pre><p>SVT-AV1 parameters:</p><pre>--preset 1 --scm 2 --keyint 63 --lookahead 0 --lp 1</pre><p>The results above demonstrate the excellent objective performance of SVT-AV1. In addition, SVT-AV1 includes implementations of some subjective quality tools, which can be used if the codec is configured for the subjective quality.</p><h4>Decoder performance</h4><p>On the objective-1-fast test set, the SVT-AV1 decoder is slightly faster than the libaom in the 1-thread mode, with larger improvements in the 4-thread mode. We observe even larger speed gains over libaom decoder when decoding bitstreams with multiple tiles using the 4-thread mode. The testing has been performed on Windows, Linux, and macOS platforms. We believe the performance is satisfactory for a research decoder, where the trade-offs favor easier experimentation over further optimizations necessary for a production decoder.</p><h3>Testing framework</h3><p>To help ensure codec conformance, especially for new code contributions, the code has been comprehensively covered with unit tests and end-to-end tests. The unit tests are built on the Google Test framework. The unit and end-to-end tests are triggered automatically for each pull request to the repository, which is supported by GitHub actions. The tests support sharding, and they run in parallel to speed-up the turn-around time on pull requests.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*vFlD0ORHr93-K2rjNF8KaA.png" /><figcaption>Unit and e2e test have passed for this pull request</figcaption></figure><h3>What’s next?</h3><p>Over the last several months, SVT-AV1 has matured to become a complete encoder/decoder package providing competitive compression efficiency and performance trade-offs. The project is bolstered with extensive unit test coverage and documentation.</p><p>Our hope is that the SVT-AV1 codebase helps further adoption of AV1 and encourages more research and development on top of the current AV1 tools. We believe that the demonstrated advantages of SVT-AV1 make it a good platform for experimentation and research. We invite colleagues from industry and academia to check out the project on Github, reach out to the codebase maintainers for questions and comments or join one of the SVT-AV1 <a href="https://github.com/OpenVisualCloud/SVT-AV1/issues/1030">Open Dev meetings</a>. We welcome more contributors to the project.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=ad295d9b5ca2" width="1" height="1"><hr><p><a href="https://netflixtechblog.com/svt-av1-an-open-source-av1-encoder-and-decoder-ad295d9b5ca2">SVT-AV1: an open-source AV1 encoder and decoder</a> was originally published in <a href="https://netflixtechblog.com">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Ready for changes with Hexagonal Architecture]]></title>
            <link>https://netflixtechblog.com/ready-for-changes-with-hexagonal-architecture-b315ec967749?source=rss----2615bd06b42e---4</link>
            <guid isPermaLink="false">https://medium.com/p/b315ec967749</guid>
            <category><![CDATA[api]]></category>
            <category><![CDATA[software-architecture]]></category>
            <category><![CDATA[hexagonal-architecture]]></category>
            <category><![CDATA[api-integration]]></category>
            <dc:creator><![CDATA[Netflix Technology Blog]]></dc:creator>
            <pubDate>Tue, 10 Mar 2020 18:34:11 GMT</pubDate>
            <atom:updated>2020-03-11T22:25:07.513Z</atom:updated>
            <content:encoded><![CDATA[<p><em>by </em><a href="https://twitter.com/DamirSvrtan"><em>Damir Svrtan</em></a><em> and </em><a href="https://twitter.com/makagon"><em>Sergii Makagon</em></a></p><p>As the production of Netflix Originals grows each year, so does our need to build apps that enable efficiency throughout the entire creative process. Our wider Studio Engineering Organization has built numerous apps that help content progress from pitch (aka screenplay) to playback: ranging from script content acquisition, deal negotiations and vendor management to scheduling, streamlining production workflows, and so on.</p><h3>Highly integrated from the start</h3><p>About a year ago, our Studio Workflows team started working on a new app that crosses multiple domains of the business. We had an interesting challenge on our hands: we needed to build the core of our app from scratch, but we also needed data that existed in many different systems.</p><p>Some of the data points we needed, such as data about movies, production dates, employees, and shooting locations, were distributed across many services implementing various protocols: gRPC, JSON API, GraphQL and more. Existing data was crucial to the behavior and business logic of our application. We needed to be highly integrated from the start.</p><h3>Swappable data sources</h3><p>One of the early applications for bringing visibility into our productions was built as a monolith. The monolith allowed for rapid development and quick changes while the knowledge of the space was non-existent. At one point, more than 30 developers were working on it, and it had well over 300 database tables.</p><p>Over time applications evolved from broad service offerings towards being highly specialized. This resulted in a decision to decompose the monolith to specific services. This decision was not geared by performance issues — but with setting boundaries around all of these different domains and enabling dedicated teams to develop domain-specific services independently.</p><p>Large amounts of the data we needed for the new app were still provided by the monolith, but we knew that the monolith would be broken up at some point. We were not sure about the timing of the breakup, but we knew that it was inevitable, and we needed to be prepared.</p><p>Thus, we could leverage some of the data from the monolith at first as it was still the source of truth, but be prepared to swap those data sources to new microservices as soon as they came online.</p><h3>Leveraging Hexagonal Architecture</h3><p>We needed to support the ability to <strong>swap data sources without impacting business logic</strong>, so we knew we needed to keep them decoupled. We decided to build our app based on principles behind <a href="https://en.wikipedia.org/wiki/Hexagonal_architecture_(software)">Hexagonal Architecture</a>.</p><p>The idea of Hexagonal Architecture is to put inputs and outputs at the edges of our design. Business logic should not depend on whether we expose a REST or a GraphQL API, and it should not depend on where we get data from — a database, a microservice API exposed via gRPC or REST, or just a simple CSV file.</p><p>The pattern allows us to isolate the core logic of our application from outside concerns. Having our core logic isolated means we can easily change data source details <strong>without a significant impact or major code rewrites to the codebase</strong>.</p><p>One of the main advantages we also saw in having an app with clear boundaries is our testing strategy — the majority of our tests can verify our business logic <strong>without relying on protocols that can easily change</strong>.</p><h3>Defining the core concepts</h3><p>Leveraged from the Hexagonal Architecture, the three main concepts that define our business logic are <strong>Entities</strong>, <strong>Repositories</strong>, and <strong>Interactors</strong>.</p><ul><li><strong>Entities</strong> are the domain objects (e.g., a Movie or a Shooting Location) — they have no knowledge of where they’re stored (unlike Active Record in Ruby on Rails or the Java Persistence API).</li><li><strong>Repositories</strong> are the interfaces to getting entities as well as creating and changing them. They keep a list of methods that are used to communicate with data sources and return a single entity or a list of entities. (e.g. UserRepository)</li><li><strong>Interactors</strong> are classes that orchestrate and perform domain actions — think of Service Objects or Use Case Objects. They implement complex business rules and validation logic specific to a domain action (e.g., onboarding a production)</li></ul><p>With these three main types of objects, we are able to define business logic without any knowledge or care where the data is kept and how business logic is triggered. Outside of the business logic are the Data Sources and the Transport Layer:</p><ul><li><strong>Data Sources</strong> are adapters to different storage implementations.<br>A data source might be an adapter to a SQL database (an Active Record class in Rails or JPA in Java), an elastic search adapter, REST API, or even an adapter to something simple such as a CSV file or a Hash. A data source implements methods defined on the repository and stores the implementation of fetching and pushing the data.</li><li><strong>Transport Layer</strong> can trigger an interactor to perform business logic. We treat it as an input for our system. The most common transport layer for microservices is the HTTP <strong>API Layer</strong> and a set of controllers that handle requests. By having business logic extracted into interactors, we are not coupled to a particular transport layer or controller implementation. Interactors can be triggered not only by a controller, but also by an event, a cron job, or from the command line.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*NfFzI7Z-E3ypn8ahESbDzw.png" /><figcaption>The dependency graph in Hexagonal Architecture goes inward.</figcaption></figure><p>With a traditional layered architecture, we would have all of our dependencies point in one direction, each layer above depending on the layer below. The transport layer would depend on the interactors, the interactors would depend on the persistence layer.</p><p>In Hexagonal Architecture all dependencies point inward — our core business logic does not know anything about the transport layer or the data sources. Still, the transport layer knows how to use interactors, and the data sources know how to conform to the repository interface.</p><p>With this, we are prepared for the inevitable changes to other Studio systems, and whenever that needs to happen, the task of swapping data sources is easy to accomplish.</p><h3>Swapping data sources</h3><p>The need to swap data sources came earlier than we expected — we suddenly hit a read constraint with the monolith and needed to switch a certain read for one entity to a newer microservice exposed over a GraphQL aggregation layer. Both the microservice and the monolith were kept in sync and had the same data, reading from one service or the other produced the same results.</p><p><strong>We managed to transfer reads from a JSON API to a GraphQL data source within 2 hours.</strong></p><p>The main reason we were able to pull it off so fast was due to the Hexagonal architecture. We didn’t let any persistence specifics leak into our business logic. We created a GraphQL data source that implemented the repository interface. A <strong>simple one-line change</strong> was all we needed to start reading from a different data source.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*uyQwbia-aIC_6kghykmM-w.png" /><figcaption>With a proper abstraction it was easy to change data sources</figcaption></figure><p>At that point, we knew that <strong>Hexagonal Architecture worked for us.</strong></p><p>The great part about a one-line change is that it mitigates risks to the release. It is very easy to rollback in the case that a downstream microservice failed on initial deployment. This as well enables us to decouple deployment and activation, as we can decide which data source to use through configuration.</p><h3>Hiding data source details</h3><p>One of the great advantages of this architecture is that we are able to encapsulate data source implementation details. We ran into a case where we needed an API call that did not yet exist — a service had an API to fetch a single resource but did not have bulk fetch implemented. After talking with the team providing the API, we realized this endpoint would take some time to deliver. So we decided to move forward with another solution to solve the problem while this endpoint was being built.</p><p>We defined a repository method that would grab multiple resources given multiple record identifiers — and the initial implementation of that method on the data source sent multiple concurrent calls to the downstream service. We knew this was a temporary solution and that the second take at the data source implementation was to use the bulk API once implemented.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*7wEa-r9iSj6Flmu1c5584w.png" /><figcaption>Our business logic doesn’t need to be aware of specific data source limitations.</figcaption></figure><p>A design like this enabled us to move forward with meeting the business needs without accruing much technical debt or the need to change any business logic afterward.</p><h3>Testing strategy</h3><p>When we started experimenting with Hexagonal Architecture, we knew we needed to come up with a testing strategy. We knew that a prerequisite to great development velocity was to have a test suite that is reliable and super fast. <strong>We didn’t think of it as a nice to have, but a must-have.</strong></p><p>We decided to test our app at three different layers:</p><ul><li>We test our <strong>interactors</strong>, where the core of our business logic lives but is independent of any type of persistence or transportation. We leverage dependency injection and mock any kind of repository interaction. This is where our business logic is <strong>tested in detail</strong>, and these are the tests we strive to have most of.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*HNRyZ_2qLppKnDZn9UtcEA.png" /></figure><ul><li>We test our <strong>data sources</strong> to determine if they integrate correctly with other services, whether they conform to the repository interface, and check how they behave upon errors. We try to minimize the amount of these tests.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Z0P4gFeqxj9ycxbM3SasYA.png" /></figure><ul><li>We have <strong>integration specs</strong> that go through the whole stack, from our Transport / API layer, through the interactors, repositories, data sources, and hit downstream services. These specs test whether we “wired” everything correctly. If a data source is an external API, we hit that endpoint and record the responses (and store them in git), allowing our test suite to run fast on every subsequent invocation. We don’t do extensive test coverage on this layer — usually just one success scenario and one failure scenario per domain action.</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*QSXoTX09D2OlkLEd44TTSg.png" /></figure><p>We don’t test our repositories as they are simple interfaces that data sources implement, and we rarely test our entities as they are plain objects with attributes defined. We test entities if they have additional methods (without touching the persistence layer).</p><p>We have room for improvement, such as not pinging any of the services we rely on but relying 100% on <a href="https://docs.pact.io/#what-is-contract-testing">contract testing</a>. With a test suite written in the above manner, we manage to run around 3000 specs in 100 seconds on a single process.</p><p>It’s lovely to work with a test suite that can easily be run on any machine, and our development team can work on their daily features without disruption.</p><h3>Delaying decisions</h3><p>We are in a great position when it comes to swapping data sources to different microservices. One of the key benefits is that we can delay some of the decisions about whether and how we want to store data internal to our application. Based on the feature’s use case, we even have the flexibility to determine the type of data store — whether it be Relational or Documents.</p><p>At the beginning of a project, we have the least amount of information about the system we are building. We should not lock ourselves into an architecture with uninformed decisions leading to a <a href="https://twitter.com/tofo/status/512666251055742977">project paradox</a>.</p><p>The decisions we made make sense for our needs now and have enabled us to move fast. The best part of Hexagonal Architecture is that it keeps our application flexible for future requirements to come.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=b315ec967749" width="1" height="1"><hr><p><a href="https://netflixtechblog.com/ready-for-changes-with-hexagonal-architecture-b315ec967749">Ready for changes with Hexagonal Architecture</a> was originally published in <a href="https://netflixtechblog.com">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
    </channel>
</rss>